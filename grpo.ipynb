{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-27 11:05:43 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.999 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.6. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-1.5b-unsloth-bnb-4bit with actual GPU utilization = 73.23%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 12.0 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 7.45 GB. Also swap space = 4 GB.\n",
      "INFO 02-27 11:06:45 config.py:549] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.self_attn', 'model.layers.0.mlp', 'model.layers.1.mlp', 'model.layers.2.mlp', 'model.layers.26.mlp', 'model.layers.15.self_attn'], 'llm_int8_threshold': 6.0}\n",
      "INFO 02-27 11:06:48 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/qwen2.5-1.5b-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-1.5b-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-1.5b-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
      "WARNING 02-27 11:06:49 interface.py:304] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 02-27 11:06:49 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 02-27 11:06:51 model_runner.py:1110] Starting to load model unsloth/qwen2.5-1.5b-unsloth-bnb-4bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W227 11:06:51.341021081 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-27 11:06:52 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 02-27 11:06:52 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a661a9627e4fc49a352617cda12204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc795c6cd1a24d9c98cb711945e0bfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-27 11:08:08 model_runner.py:1115] Loading model weights took 1.3190 GB\n",
      "INFO 02-27 11:08:08 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 02-27 11:08:13 worker.py:267] Memory profiling takes 4.62 seconds\n",
      "INFO 02-27 11:08:13 worker.py:267] the current vLLM instance can use total_gpu_memory (12.00GiB) x gpu_memory_utilization (0.73) = 8.79GiB\n",
      "INFO 02-27 11:08:13 worker.py:267] model weights take 1.32GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 1.05GiB; the rest of the memory reserved for KV Cache is 6.39GiB.\n",
      "INFO 02-27 11:08:13 executor_base.py:111] # cuda blocks: 14957, # CPU blocks: 9362\n",
      "INFO 02-27 11:08:13 executor_base.py:116] Maximum concurrency for 1024 tokens per request: 233.70x\n",
      "INFO 02-27 11:08:14 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-27 11:08:34 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.84 GiB\n",
      "INFO 02-27 11:08:34 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 25.42 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.2.15 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
    "lora_rank = 64 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-1.5B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.8, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # Remove QKVO if out of memory\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"\"\"{% for message in messages %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + message['content'] + '<|im_end|>\\n' }}{% elif message['role'] == 'assistant' %}{{ '<|im_start|>assistant\\n' + message['content'] + '<|im_end|>\\n' }}{% elif message['role'] == 'system' %}{{ '<|im_start|>system\\n' + message['content'] + '<|im_end|>\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "dataset = get_gsm8k_questions()\n",
    "\n",
    "# Reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    # print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 4\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference!\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 20,\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = 512,\n",
    "    max_completion_length = 256,\n",
    "    num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    # max_steps = 250,\n",
    "    save_steps = 250,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/WSL2/Ubuntu/home/yuiseki/miniconda_envs/unsloth/lib/python3.11/site-packages/transformers/trainer.py:3423: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 7,473 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 4 | Total steps = 7,473\n",
      " \"-____-\"     Number of trainable parameters = 73,859,072\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7473' max='7473' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7473/7473 : < :, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completion_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / xmlcount_reward_func</th>\n",
       "      <th>rewards / soft_format_reward_func</th>\n",
       "      <th>rewards / strict_format_reward_func</th>\n",
       "      <th>rewards / int_reward_func</th>\n",
       "      <th>rewards / correctness_reward_func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7473, training_loss=0.0, metrics={'train_runtime': 0.0068, 'train_samples_per_second': 1102304.688, 'train_steps_per_second': 1102304.688, 'total_flos': 0.0, 'train_loss': 0.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.00s/it, est. speed input: 1.60 toks/s, output: 100.86 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How many r's are in strawberry?èµ¸\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸assistant\\nèµ¸\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"How many r's are in strawberry?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.30s/it, est. speed input: 4.62 toks/s, output: 105.59 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'çŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒžçŒž'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : \"How many r's are in strawberry?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('outputs/YuisekinAI-Qwen2.5-1.5B-GRPO-GSM8K/tokenizer_config.json',\n",
       " 'outputs/YuisekinAI-Qwen2.5-1.5B-GRPO-GSM8K/special_tokens_map.json',\n",
       " 'outputs/YuisekinAI-Qwen2.5-1.5B-GRPO-GSM8K/vocab.json',\n",
       " 'outputs/YuisekinAI-Qwen2.5-1.5B-GRPO-GSM8K/merges.txt',\n",
       " 'outputs/YuisekinAI-Qwen2.5-1.5B-GRPO-GSM8K/added_tokens.json',\n",
       " 'outputs/YuisekinAI-Qwen2.5-1.5B-GRPO-GSM8K/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"outputs/YuisekinAI-Qwen2.5-1.5B-GRPO-GSM8K\"\n",
    "model.save_pretrained(checkpoint_path)\n",
    "tokenizer.save_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub_gguf(\n",
    "        \"yuiseki/YuisekinAI-Qwen2.5-1.5B-GRPO-GSM8K-GGUF\",\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
